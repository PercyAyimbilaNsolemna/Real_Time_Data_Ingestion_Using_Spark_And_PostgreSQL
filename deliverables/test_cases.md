<div align="center">

# Test Cases Documentation
## Spark Streaming to PostgreSQL Pipeline

*Comprehensive validation of correctness, reliability, and robustness*

---

[![Test Coverage](https://img.shields.io/badge/Coverage-6%20Cases-success?style=flat-square)]()
[![Status](https://img.shields.io/badge/Status-All%20Passed-brightgreen?style=flat-square)]()
[![Manual Testing](https://img.shields.io/badge/Type-Manual-blue?style=flat-square)]()

</div>

---

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Test Environment](#test-environment)
- [Test Cases](#test-cases)
  - [TC-01: End-to-End Ingestion](#test-case-1-successful-end-to-end-data-ingestion)
  - [TC-02: Bad Record Handling](#test-case-2-handling-of-malformed--bad-records)
  - [TC-03: Graceful Shutdown](#test-case-3-graceful-shutdown-of-streaming-job)
  - [TC-04: Connection Failure](#test-case-4-postgresql-connectivity-failure)
  - [TC-05: Duplicate Events](#test-case-5-duplicate-event-handling)
  - [TC-06: Checkpoint Recovery](#test-case-6-streaming-restart-with-checkpoint-recovery)
- [Test Summary](#summary)

---

## Overview

This document outlines the manual test cases used to validate the correctness, reliability, and robustness of the Spark Structured Streaming pipeline that ingests CSV event data and writes it into PostgreSQL.

### Testing Scope

<table>
<tr>
<th width="30%">Category</th>
<th width="70%">Coverage</th>
</tr>
<tr>
<td><strong>Functional</strong></td>
<td>End-to-end data ingestion, validation, and persistence</td>
</tr>
<tr>
<td><strong>Error Handling</strong></td>
<td>Malformed records, connection failures, invalid data</td>
</tr>
<tr>
<td><strong>Reliability</strong></td>
<td>Checkpoint recovery, graceful shutdown, duplicate handling</td>
</tr>
<tr>
<td><strong>Integration</strong></td>
<td>Spark-PostgreSQL connectivity, Docker containerization</td>
</tr>
</table>

---

## Test Environment

### System Configuration

<table>
<tr>
<th>Component</th>
<th>Version/Details</th>
</tr>
<tr>
<td>Apache Spark</td>
<td>3.5.x (Structured Streaming)</td>
</tr>
<tr>
<td>PostgreSQL</td>
<td>16.x</td>
</tr>
<tr>
<td>Docker</td>
<td>24.x with Docker Compose v2</td>
</tr>
<tr>
<td>Python</td>
<td>3.8+</td>
</tr>
<tr>
<td>Test Method</td>
<td>Manual execution with observation</td>
</tr>
</table>

---

## Test Cases

---

### Test Case 1: Successful End-to-End Data Ingestion

<table>
<tr>
<th width="25%">Attribute</th>
<th width="75%">Details</th>
</tr>
<tr>
<td><strong>Test ID</strong></td>
<td>TC-01</td>
</tr>
<tr>
<td><strong>Priority</strong></td>
<td>ðŸ”´ Critical</td>
</tr>
<tr>
<td><strong>Category</strong></td>
<td>Functional - Happy Path</td>
</tr>
</table>

#### Objective

Verify that valid CSV events generated by `data_generator.py` are successfully processed by Spark and written into the PostgreSQL table.

#### Preconditions

- âœ“ Docker containers (Postgres, Spark) are running
- âœ“ PostgreSQL database and tables are created using `postgres_setup.sql`
- âœ“ Spark streaming job is not running

#### Test Steps

<table>
<tr>
<th width="10%">Step</th>
<th width="90%">Action</th>
</tr>
<tr>
<td><strong>1</strong></td>
<td>
Start the data generator script:

```bash
python app/data_generator.py
```
</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>
Start the Spark streaming job:

```bash
python app/spark_streaming_to_postgres.py
```
</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>Allow the system to run for at least 2 minutes.</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>
Open pgAdmin and query the events table:

```sql
SELECT COUNT(*) FROM ecommerce_events;
```
</td>
</tr>
</table>

#### Expected Result

- âœ“ Records appear in the `ecommerce_events` table
- âœ“ Record count increases over time
- âœ“ No Spark job failures
- âœ“ Logs show successful batch processing

#### Actual Result

- âœ“ Records inserted successfully
- âœ“ Streaming job remains active
- âœ“ Batch processing logs show continuous ingestion

#### Validation Queries

```sql
-- Verify record count growth
SELECT COUNT(*) FROM ecommerce_events;

-- Check latest records
SELECT * FROM ecommerce_events 
ORDER BY ingestion_timestamp DESC 
LIMIT 10;

-- Validate event type distribution
SELECT event_type, COUNT(*) 
FROM ecommerce_events 
GROUP BY event_type;
```

#### Status

<div align="center">

**âœ… PASSED**

</div>

---

### Test Case 2: Handling of Malformed / Bad Records

<table>
<tr>
<th width="25%">Attribute</th>
<th width="75%">Details</th>
</tr>
<tr>
<td><strong>Test ID</strong></td>
<td>TC-02</td>
</tr>
<tr>
<td><strong>Priority</strong></td>
<td>ðŸ”´ Critical</td>
</tr>
<tr>
<td><strong>Category</strong></td>
<td>Error Handling - Data Quality</td>
</tr>
</table>

#### Objective

Ensure that malformed CSV records are redirected to the bad-records sink and do not crash the streaming job.

#### Preconditions

- âœ“ Spark streaming job is configured with `badRecordsPath`
- âœ“ Data generator can emit malformed rows

#### Test Steps

<table>
<tr>
<th width="10%">Step</th>
<th width="90%">Action</th>
</tr>
<tr>
<td><strong>1</strong></td>
<td>Modify the data generator to emit malformed CSV rows (missing fields or invalid datatypes).</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>Start the Spark streaming job.</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>Monitor Spark logs and bad-records directory.</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>
Verify bad records isolation:

```bash
ls -la data/bad_records/
cat data/bad_records/part-*.csv
```
</td>
</tr>
</table>

#### Expected Result

- âœ“ Valid records are written to PostgreSQL
- âœ“ Invalid records are written to the bad-records directory
- âœ“ Streaming job continues running without interruption
- âœ“ No data loss for valid records

#### Actual Result

- âœ“ Bad records logged separately in `data/bad_records/`
- âœ“ No job termination observed
- âœ“ Valid records processed normally

#### Validation

```bash
# Count bad records
find data/bad_records/ -name "part-*" -type f -exec wc -l {} \;

# Verify valid records still in PostgreSQL
docker exec -it postgres psql -U postgres -d ecommerce \
  -c "SELECT COUNT(*) FROM ecommerce_events;"
```

#### Status

<div align="center">

**âœ… PASSED**

</div>

---

### Test Case 3: Graceful Shutdown of Streaming Job

<table>
<tr>
<th width="25%">Attribute</th>
<th width="75%">Details</th>
</tr>
<tr>
<td><strong>Test ID</strong></td>
<td>TC-03</td>
</tr>
<tr>
<td><strong>Priority</strong></td>
<td>ðŸŸ¡ High</td>
</tr>
<tr>
<td><strong>Category</strong></td>
<td>Reliability - Shutdown Behavior</td>
</tr>
</table>

#### Objective

Verify that interrupting the Spark job (Ctrl+C) shuts down gracefully and cleans up resources.

#### Preconditions

- âœ“ Spark streaming job is running
- âœ“ Data is actively being processed

#### Test Steps

<table>
<tr>
<th width="10%">Step</th>
<th width="90%">Action</th>
</tr>
<tr>
<td><strong>1</strong></td>
<td>Start the Spark streaming job.</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>Allow it to process data for at least 30 seconds.</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>Press <code>Ctrl+C</code> in the terminal.</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>Observe shutdown logs and verify no zombie processes.</td>
</tr>
</table>

#### Expected Result

- âœ“ Streaming queries stop gracefully
- âœ“ Spark session shuts down cleanly
- âœ“ Logs indicate orderly shutdown sequence
- âœ“ No checkpoint corruption
- âœ“ Resources released properly

#### Actual Result

- âœ“ Logs show orderly shutdown sequence
- âœ“ Terminal shows `KeyboardInterrupt` traceback (expected behavior)
- âœ“ No orphaned processes remain

#### Log Verification

```bash
# Check for graceful shutdown messages
tail -n 50 logs/spark_streaming.log | grep -i shutdown

# Verify no zombie processes
docker exec -it spark ps aux | grep spark
```

#### Status

<div align="center">

**âœ… PASSED** *(expected interruption behavior)*

</div>

---


## Summary

<div align="center">

### Test Execution Summary

</div>

<table>
<tr>
<th width="40%">Metric</th>
<th width="60%">Result</th>
</tr>
<tr>
<td><strong>Total Test Cases</strong></td>
<td>6</td>
</tr>
<tr>
<td><strong>Passed</strong></td>
<td>6 (100%)</td>
</tr>
<tr>
<td><strong>Failed</strong></td>
<td>0</td>
</tr>
<tr>
<td><strong>Blocked</strong></td>
<td>0</td>
</tr>
<tr>
<td><strong>Test Coverage</strong></td>
<td>Functional, Error Handling, Reliability, Integration</td>
</tr>
</table>

### Coverage by Category

<table>
<tr>
<th>Category</th>
<th>Test Cases</th>
<th>Status</th>
</tr>
<tr>
<td><strong>Functional Testing</strong></td>
<td>TC-01, TC-05</td>
<td>âœ… Passed</td>
</tr>
<tr>
<td><strong>Error Handling</strong></td>
<td>TC-02, TC-04</td>
<td>âœ… Passed</td>
</tr>
<tr>
<td><strong>Reliability</strong></td>
<td>TC-03, TC-06</td>
<td>âœ… Passed</td>
</tr>
</table>

### Key Findings

All critical system behaviorsâ€”including ingestion, error handling, recovery, and shutdownâ€”were tested manually. The pipeline demonstrates:

```
âœ“ Reliable end-to-end data ingestion
âœ“ Robust error handling with bad record isolation
âœ“ Graceful shutdown behavior
âœ“ Clear error messaging for infrastructure failures
âœ“ Predictable duplicate handling
âœ“ Successful checkpoint-based recovery
```

### Recommendations

1. **Automated Testing:** Consider implementing automated test suite for regression testing
2. **Performance Testing:** Add load testing for high-volume scenarios
3. **Deduplication:** Evaluate if deduplication logic should be added
4. **Monitoring:** Integrate with monitoring tools (Prometheus, Grafana) for production deployment

---

<div align="center">

**Testing completed on:** January 30, 2024  
**Tested by:** Percy Ayimbila Nsolemna  
**Version:** 1.0.0

---

*For questions or issues, refer to the [User Guide](USER_GUIDE.md) or [Main README](../README.md)*

</div>